{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/birdx0810/LearningTensorFlow/blob/main/TensorFlow___From_1_To_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBn8NWTYy7Xr"
   },
   "source": [
    "# From TensorFlow 1.x to 2.x and PyTorch\n",
    "This is notebook is basically an implementation of a basic multi-layer perceptron using TF1.0, TF2.0 and PyTorch method for MNIST dataset classification. The results should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x7TeNpn9yMh5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Import MINST data \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tqdm import trange\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pb2S3hVmyrTj"
   },
   "outputs": [],
   "source": [
    "# Set Model Parameters\n",
    "learning_rate = 1e-5 \n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qFGqosW4n9XJ"
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ck4Rg_JoM6fh"
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1]*train_x.shape[2])\n",
    "test_x = test_x.reshape(test_x.shape[0], test_x.shape[1]*test_x.shape[2])\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(num_labels))\n",
    "\n",
    "train_onehot = label_binarizer.transform(train_y)\n",
    "test_onehot = label_binarizer.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "X_dT9dCynUYx",
    "outputId": "e4ac234e-1171-4546-cd34-e6b956e64e90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GCPCRVkaIGNx"
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, iteration, batch_size):\n",
    "    start = iteration * batch_size\n",
    "    end = start + batch_size\n",
    "\n",
    "    x_mb = x[start:end]\n",
    "    y_mb = y[start:end]\n",
    "  \n",
    "    return x_mb, y_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg-z-_INzmfZ"
   },
   "source": [
    "## TensorFlow 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "9hXdrN8iz1Sf",
    "outputId": "06b903ff-0c69-492b-a9c7-7df43dce0b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bird/Documents/Code/LearningTensorFlow/venv/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.set_random_seed(42)\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# tf Graph Input \n",
    "x = tf.placeholder(\"float\", [None, 784])  # mnist data image of shape 28*28 = 784 \n",
    "y = tf.placeholder(\"float\", [None, 10])   # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weight and bias \n",
    "W = tf.Variable(tf.ones([784, 10]))      # 784 -> 10 \n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "logits = tf.matmul(x, W) + b\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "\n",
    "pred = tf.nn.softmax(logits)\n",
    "acc, acc_op = tf.metrics.accuracy(labels=tf.argmax(y,1), predictions=tf.argmax(pred,1))\n",
    "\n",
    "# Optimize model using gradient descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "m24rX6aR1CEb",
    "outputId": "063f7069-14f8-4ee5-d78d-c323d1a0e7af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1234.71it/s]\n",
      "  7%|▋         | 135/1875 [00:00<00:01, 1343.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:1\ttotal loss=0.38425698899527416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1360.42it/s]\n",
      "  7%|▋         | 139/1875 [00:00<00:01, 1385.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:2\ttotal loss=0.2992388392756381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1392.97it/s]\n",
      "  7%|▋         | 139/1875 [00:00<00:01, 1378.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:3\ttotal loss=0.2879544903546581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1377.74it/s]\n",
      "  8%|▊         | 142/1875 [00:00<00:01, 1412.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:4\ttotal loss=0.2816752713834249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1437.34it/s]\n",
      "  7%|▋         | 133/1875 [00:00<00:01, 1319.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:5\ttotal loss=0.2773895827760295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1356.52it/s]\n",
      "  7%|▋         | 137/1875 [00:00<00:01, 1360.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:6\ttotal loss=0.27416983421295893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1380.71it/s]\n",
      " 14%|█▍        | 271/1875 [00:00<00:01, 1345.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:7\ttotal loss=0.27163260131428585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1340.38it/s]\n",
      " 14%|█▍        | 271/1875 [00:00<00:01, 1356.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:8\ttotal loss=0.26952694824635987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1369.76it/s]\n",
      "  8%|▊         | 141/1875 [00:00<00:01, 1408.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:9\ttotal loss=0.26775324675391127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:01<00:00, 1424.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:10\ttotal loss=0.26621821887244795\n",
      "model acc: 0.9172999858856201\n",
      "Time taken: 13.767048120498657 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables \n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()\n",
    "\n",
    "# Launch the graph \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_global)\n",
    "    sess.run(init_local)\n",
    "  \n",
    "    # Training cycle\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.\n",
    "        iterations = int(len(train_x)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in trange(iterations):\n",
    "            # Get mini batch\n",
    "            batch_xs, batch_ys = get_batch(train_x, train_onehot, i, batch_size)\n",
    "      \n",
    "            # Fit training using batch data \n",
    "            _, batch_loss = sess.run([optimizer, loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "            total_loss += batch_loss/iterations\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        total_loss = total_loss\n",
    "        print(f\"\\nEpoch:{epoch+1}\\ttotal loss={total_loss}\")\n",
    "    \n",
    "    _, accuracy = sess.run([acc, acc_op], feed_dict={x: test_x, y: test_onehot})\n",
    "    print(f\"model acc: {accuracy}\")\n",
    "    print(f\"Time taken: {time.time() - start} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc2iEcemjn6u"
   },
   "source": [
    "## TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "ho7shoW-jx8_",
    "outputId": "272a2310-aec1-4d77-fdbf-42ad3c53cd7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# First define the model \n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add the dense layer to the model\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=num_labels, \n",
    "    activation=\"softmax\", \n",
    "    input_shape=(784,), \n",
    "    weights=[np.ones([784, 10]), np.zeros(10)]\n",
    "))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZTLPCpi1tiCs"
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "criterion = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"], loss=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "7Vv5LQSvtt_Q",
    "outputId": "0be2858f-800b-45e8-9517-2bcfbcb51b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.3768 - acc: 0.8963\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2908 - acc: 0.9184\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2794 - acc: 0.9230\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2739 - acc: 0.9241\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2694 - acc: 0.9262\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2656 - acc: 0.9268\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2634 - acc: 0.9273\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.2616 - acc: 0.9275\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2589 - acc: 0.9286\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2578 - acc: 0.9286\n",
      "Time taken: 21.84522318840027 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_history = model.fit(\n",
    "    train_x, train_onehot, \n",
    "    batch_size=batch_size, epochs=epochs\n",
    ")\n",
    "print(f\"Time taken: {time.time() - start} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "Z4jmmiVZvVnR",
    "outputId": "9166fbbb-1eb4-46e2-f874-2937b647c96c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bird/Documents/Code/LearningTensorFlow/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Acc: 0.9207000136375427\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_x, test_onehot)\n",
    "print(f\"Acc: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kejSzQ21zuRR"
   },
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HgY0Anlb1MZ2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(784, 10)\n",
    "        torch.nn.init.ones_(self.linear.weight)\n",
    "        torch.nn.init.zeros_(self.linear.bias)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        logits = self.linear(X)\n",
    "        pred = self.softmax(logits)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]<ipython-input-31-5db9b7eb952f>:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = self.softmax(logits)\n",
      "100%|██████████| 1875/1875 [00:00<00:00, 4358.79it/s]\n",
      " 25%|██▍       | 465/1875 [00:00<00:00, 4643.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:1\ttotal loss=1.6154459714889526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4609.63it/s]\n",
      " 25%|██▍       | 466/1875 [00:00<00:00, 4650.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:2\ttotal loss=1.5644217729568481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4502.07it/s]\n",
      " 25%|██▍       | 460/1875 [00:00<00:00, 4592.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:3\ttotal loss=1.5561379194259644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4491.37it/s]\n",
      " 23%|██▎       | 439/1875 [00:00<00:00, 4382.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:4\ttotal loss=1.551438331604004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4436.39it/s]\n",
      " 24%|██▎       | 443/1875 [00:00<00:00, 4423.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:5\ttotal loss=1.5481897592544556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4438.09it/s]\n",
      " 22%|██▏       | 411/1875 [00:00<00:00, 4107.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:6\ttotal loss=1.5457316637039185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4220.46it/s]\n",
      " 24%|██▍       | 452/1875 [00:00<00:00, 4511.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:7\ttotal loss=1.5437616109848022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4487.65it/s]\n",
      " 48%|████▊     | 891/1875 [00:00<00:00, 4438.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:8\ttotal loss=1.5421299934387207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4433.93it/s]\n",
      " 24%|██▍       | 446/1875 [00:00<00:00, 4453.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:9\ttotal loss=1.5407434701919556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:00<00:00, 4421.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:10\ttotal loss=1.539556860923767\n",
      "Time taken: 4.27206826210022 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.\n",
    "    iterations = int(len(train_x)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in trange(iterations):\n",
    "        # Get mini batch\n",
    "        batch_xs, batch_ys = get_batch(train_x, train_y, i, batch_size)\n",
    "        \n",
    "        batch_xs = torch.FloatTensor(batch_xs)\n",
    "        batch_ys = torch.LongTensor(batch_ys)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch_ps = model(batch_xs)\n",
    "        loss = criterion(batch_ps, batch_ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "    print(f\"\\nEpoch:{epoch+1}\\ttotal loss={total_loss/iterations}\")\n",
    "    \n",
    "print(f\"Time taken: {time.time() - start} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-5db9b7eb952f>:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = self.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_x = torch.FloatTensor(test_x)\n",
    "test_p = model(test_x)\n",
    "test_p = test_p.detach().numpy().argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9256\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_y, test_p)\n",
    "print(f\"Acc: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPatHrElR+0RemT8q3wAOv0",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TensorFlow_-_From_1_To_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
