{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_-_From_1_To_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHKcbG239ATRczPuDenS0b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBn8NWTYy7Xr"
      },
      "source": [
        "# TensorFlow: From 1.x to 2.x\n",
        "\n",
        "This is notebook is basically an implementation of a basic multi-layer perceptron using TF1.0 and TF2.0 method for MNIST dataset classification. The results should be similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7TeNpn9yMh5"
      },
      "source": [
        "import time\n",
        "\n",
        "# Import MINST data \n",
        "from keras.datasets import mnist\n",
        "from tqdm import trange\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb2S3hVmyrTj"
      },
      "source": [
        "# Set Model Parameters\n",
        "learning_rate = 1e-5 \n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "num_labels = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFGqosW4n9XJ"
      },
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck4Rg_JoM6fh"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1]*train_x.shape[2])\n",
        "test_x = test_x.reshape(test_x.shape[0], test_x.shape[1]*test_x.shape[2])\n",
        "\n",
        "label_binarizer = LabelBinarizer()\n",
        "label_binarizer.fit(range(num_labels))\n",
        "\n",
        "train_y = label_binarizer.transform(train_y)\n",
        "test_y = label_binarizer.transform(test_y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCPCRVkaIGNx"
      },
      "source": [
        "def get_batch(x, y, iteration, batch_size):\n",
        "  \n",
        "  start = iteration * batch_size\n",
        "  end = start + batch_size\n",
        "\n",
        "  x_mb = x[start:end]\n",
        "  y_mb = y[start:end]\n",
        "  \n",
        "  return x_mb, y_mb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg-z-_INzmfZ"
      },
      "source": [
        "## TensorFlow 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hXdrN8iz1Sf"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# tf Graph Input \n",
        "x = tf.placeholder(\"float\", [None, 784])  # mnist data image of shape 28*28 = 784 \n",
        "y = tf.placeholder(\"float\", [None, 10])   # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# Set model weight and bias \n",
        "W = tf.Variable(tf.zeros([784, 10]))      # 784 -> 10 \n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "logits = tf.matmul(x, W) + b\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
        "\n",
        "pred = tf.nn.softmax(logits)\n",
        "acc, acc_op = tf.metrics.accuracy(labels=tf.argmax(y,1), predictions=tf.argmax(pred,1))\n",
        "\n",
        "# Optimize model using gradient descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24rX6aR1CEb",
        "outputId": "7125908d-8b5e-47ed-b05a-6c12da4775df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initializing the variables \n",
        "init_global = tf.global_variables_initializer()\n",
        "init_local = tf.local_variables_initializer()\n",
        "\n",
        "# Launch the graph \n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_global)\n",
        "  sess.run(init_local)\n",
        "  \n",
        "  # Training cycle\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0.\n",
        "    iterations = int(len(train_x)/batch_size)\n",
        "    c = 0\n",
        "    if True:\n",
        "    # Loop over all batches\n",
        "      for i in trange(iterations):\n",
        "        # Get mini batch\n",
        "        batch_xs, batch_ys = get_batch(train_x, train_y, i, batch_size)\n",
        "        \n",
        "        # Fit training using batch data \n",
        "        _, batch_loss = sess.run([optimizer, loss], feed_dict={x: batch_xs, y: batch_ys})\n",
        "      \n",
        "        total_loss += batch_loss/iterations\n",
        "    else:\n",
        "        # Fit training using batch data \n",
        "        _, total_loss = sess.run([optimizer, loss], feed_dict={x: train_x, y: train_y})\n",
        "\n",
        "    # Display logs per epoch step\n",
        "    total_loss = total_loss\n",
        "    print(f\"\\nEpoch:{epoch+1}\\ttotal lost={total_loss}\")\n",
        "    time.sleep(.25)\n",
        "\n",
        "  accuracy, acc_ops = sess.run([acc, acc_op], feed_dict={x: test_x, y: test_y})\n",
        "  print(f\"Acc: {acc_ops}\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1299.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:1\ttotal lost=0.39730687200799597\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1333.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:2\ttotal lost=0.3390337300014986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1340.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:3\ttotal lost=0.32765584391007796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1344.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:4\ttotal lost=0.3209940871874489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1343.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:5\ttotal lost=0.31636661525666704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1312.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:6\ttotal lost=0.3128744418690606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1342.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:7\ttotal lost=0.3101038516620793\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1317.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:8\ttotal lost=0.3078271562347812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1334.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:9\ttotal lost=0.30590681703587336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1336.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:10\ttotal lost=0.3042548801978427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1338.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:11\ttotal lost=0.30281209012543153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1350.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:12\ttotal lost=0.301536564977964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1345.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:13\ttotal lost=0.30039755644798205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1335.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:14\ttotal lost=0.29937175755550394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1316.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:15\ttotal lost=0.2984410687449082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1344.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:16\ttotal lost=0.2975911665047217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1322.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:17\ttotal lost=0.29681056584020493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1323.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:18\ttotal lost=0.2960899222118164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1320.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:19\ttotal lost=0.2954215780769782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:01<00:00, 1315.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:20\ttotal lost=0.29479917484199\n",
            "Acc: 0.9009000062942505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc2iEcemjn6u"
      },
      "source": [
        "## TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho7shoW-jx8_",
        "outputId": "fe6f61d0-2b38-4920-ed75-2f60e75915de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Import relevant packages\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "# First define the model \n",
        "model = Sequential()\n",
        "\n",
        "# Add the dense layer to the model\n",
        "model.add(tf.keras.layers.Dense(units=num_labels, activation=\"softmax\", input_shape=(784,)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTLPCpi1tiCs"
      },
      "source": [
        "# Define loss and optimizer\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "criterion = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'], loss=criterion)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vv5LQSvtt_Q",
        "outputId": "d37fb5f3-621f-4402-a5f0-607658958a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "train_history = model.fit(\n",
        "  train_x, train_y, \n",
        "  batch_size=batch_size, epochs=epochs\n",
        ")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 12.6401 - acc: 0.2154\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 11.6534 - acc: 0.2768\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 11.6158 - acc: 0.2792\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5899 - acc: 0.2808\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5728 - acc: 0.2819\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5884 - acc: 0.2809\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5998 - acc: 0.2802\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5887 - acc: 0.2809\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5591 - acc: 0.2828\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5472 - acc: 0.2836\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5550 - acc: 0.2831\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5983 - acc: 0.2803\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5553 - acc: 0.2830\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5398 - acc: 0.2840\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5428 - acc: 0.2838\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5536 - acc: 0.2831\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5422 - acc: 0.2839\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5620 - acc: 0.2826\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5537 - acc: 0.2831\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 11.5513 - acc: 0.2833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4jmmiVZvVnR",
        "outputId": "6314e810-1fc6-475b-c435-0f8da98f5078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(test_x, test_y)\n",
        "print(f\"Acc: {accuracy}\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.27889999747276306\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}